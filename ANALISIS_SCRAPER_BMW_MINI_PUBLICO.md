# üîç AN√ÅLISIS T√âCNICO: SCRAPERS BMW PREMIUM SELECTION Y MINI NEXT

## üìã OBJETIVO
Scrapear las webs p√∫blicas de BMW Premium Selection y MINI Next para comparar precios y extras con nuestro stock.

---

## üåê URLs OBJETIVO

### 1. BMW Premium Selection Espa√±a
```
URL Principal: https://www.bmw.es/es/topics/offers-and-services/bmw-premium-selection/search.html
Descripci√≥n: Buscador de veh√≠culos ocasi√≥n certificados BMW en toda Espa√±a
Veh√≠culos: ~500-800 unidades (estimado)
```

### 2. MINI Next Espa√±a
```
URL Principal: https://www.mininext.es/buscador-avanzado
Descripci√≥n: Buscador avanzado de veh√≠culos ocasi√≥n certificados MINI en toda Espa√±a
Veh√≠culos: ~200-400 unidades (estimado)
```

---

## üî¨ AN√ÅLISIS T√âCNICO

### Caracter√≠sticas Comunes
- ‚úÖ **Contenido din√°mico**: Ambas webs cargan datos con JavaScript
- ‚úÖ **Paginaci√≥n**: Resultados distribuidos en m√∫ltiples p√°ginas
- ‚úÖ **Filtros avanzados**: Modelo, a√±o, precio, km, ubicaci√≥n
- ‚úÖ **Fichas detalladas**: Cada veh√≠culo tiene p√°gina individual con extras
- ‚ö†Ô∏è **Anti-scraping**: Posibles medidas de protecci√≥n (captcha, rate limiting)

### Tecnolog√≠a Requerida
```python
Herramientas necesarias:
‚îú‚îÄ‚îÄ Selenium ‚Üí Renderizar JavaScript y navegaci√≥n din√°mica
‚îú‚îÄ‚îÄ BeautifulSoup ‚Üí Parsear HTML extra√≠do
‚îú‚îÄ‚îÄ Pandas ‚Üí Estructurar datos extra√≠dos
‚îú‚îÄ‚îÄ Supabase Python Client ‚Üí Upload a base de datos
‚îî‚îÄ‚îÄ Chrome WebDriver ‚Üí Navegador headless
```

---

## üèóÔ∏è ARQUITECTURA DEL SCRAPER

### Fase 1: Navegaci√≥n y Listado
```python
1. Iniciar Chrome con Selenium (headless mode)
2. Acceder a buscador
3. Aplicar filtros (opcional: solo BMW/MINI espec√≠ficos)
4. Extraer n√∫mero total de resultados
5. Calcular n√∫mero de p√°ginas
6. Iterar por cada p√°gina
```

### Fase 2: Extracci√≥n de Listado
```python
Para cada p√°gina:
  1. Esperar carga completa (wait for elements)
  2. Extraer elementos de veh√≠culos
  3. Para cada veh√≠culo en listado:
     - ID del anuncio
     - Modelo b√°sico
     - Precio
     - Km
     - A√±o
     - URL ficha detallada
     - Imagen principal
     - Ubicaci√≥n (concesionario)
```

### Fase 3: Extracci√≥n Detallada
```python
Para cada URL de ficha:
  1. Acceder a ficha individual
  2. Extraer datos completos:
     ‚îú‚îÄ‚îÄ Versi√≥n completa
     ‚îú‚îÄ‚îÄ Precio detallado
     ‚îú‚îÄ‚îÄ Lista completa de extras
     ‚îú‚îÄ‚îÄ Equipamiento de serie
     ‚îú‚îÄ‚îÄ Color exterior
     ‚îú‚îÄ‚îÄ Color interior
     ‚îú‚îÄ‚îÄ Tipo combustible
     ‚îú‚îÄ‚îÄ Transmisi√≥n
     ‚îú‚îÄ‚îÄ Potencia (CV)
     ‚îú‚îÄ‚îÄ Matr√≠cula (si visible)
     ‚îú‚îÄ‚îÄ VIN (si visible)
     ‚îú‚îÄ‚îÄ Todas las fotos
     ‚îú‚îÄ‚îÄ Nombre concesionario
     ‚îî‚îÄ‚îÄ Ubicaci√≥n completa
```

### Fase 4: Almacenamiento
```python
1. Limpiar y normalizar datos
2. Convertir a formato est√°ndar
3. Upload a Supabase (tablas correspondientes)
4. Actualizar timestamp de scraping
5. Log de resultados
```

---

## üìä ESTRUCTURA DE DATOS A EXTRAER

### Datos M√≠nimos (del Listado)
```python
{
  "listing_id": "BMW_12345",           # ID √∫nico del anuncio
  "model": "BMW Serie 3 320d",
  "price": 32990,
  "mileage": 45000,
  "year": 2022,
  "url": "https://...",
  "main_image": "https://...",
  "dealership_location": "Madrid"
}
```

### Datos Completos (de la Ficha)
```python
{
  "listing_id": "BMW_12345",
  "license_plate": "1234ABC",          # Si visible
  "model": "BMW Serie 3",
  "version": "320d M Sport",
  "year": 2022,
  "mileage": 45000,
  "price": 32990,
  
  # T√©cnicos
  "fuel_type": "Di√©sel",
  "transmission": "Autom√°tico",
  "power_hp": 190,
  "doors": 4,
  "seats": 5,
  "body_type": "Berlina",
  
  # Est√©tica
  "exterior_color": "Gris Mineral Metalizado",
  "interior_color": "Cuero Vernasca Negro",
  
  # Equipamiento
  "extras": [
    "Navegaci√≥n Professional",
    "Techo solar panor√°mico",
    "Asientos calefactables",
    "Sistema HiFi Harman Kardon",
    "Park Assist",
    "C√°mara trasera"
  ],
  "standard_equipment": [
    "Control crucero adaptativo",
    "Faros LED",
    "Climatizador autom√°tico"
  ],
  
  # Ubicaci√≥n
  "dealership_name": "BMW Madrid Norte",
  "dealership_location": "Madrid",
  "dealership_address": "Calle ...",
  
  # Media
  "images": [
    "https://image1.jpg",
    "https://image2.jpg",
    ...
  ],
  
  # Metadatos
  "url": "https://ficha-completa",
  "scraped_at": "2025-10-27T10:00:00Z",
  "last_seen_at": "2025-10-27T10:00:00Z"
}
```

---

## üõ†Ô∏è IMPLEMENTACI√ìN T√âCNICA

### Scraper BMW Premium Selection

```python
# scrapers/bmw_premium_selection_scraper.py

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import json

class BMWPremiumSelectionScraper:
    def __init__(self):
        self.base_url = "https://www.bmw.es/es/topics/offers-and-services/bmw-premium-selection/search.html"
        self.driver = None
        self.vehicles = []
    
    def init_driver(self):
        """Inicializar Chrome en modo headless"""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.implicitly_wait(10)
    
    def scrape_listings(self):
        """Extraer listado completo de veh√≠culos"""
        self.driver.get(self.base_url)
        
        # Esperar a que carguen los resultados
        WebDriverWait(self.driver, 20).until(
            EC.presence_of_element_located((By.CLASS_NAME, "vehicle-card"))
        )
        
        # TODO: Identificar selectores reales inspeccionando la web
        # Estos son selectores de ejemplo
        
        page = 1
        while True:
            print(f"üìÑ Scrapeando p√°gina {page}...")
            
            # Extraer veh√≠culos de la p√°gina actual
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            vehicle_cards = soup.find_all('div', class_='vehicle-card')
            
            for card in vehicle_cards:
                vehicle = self.extract_listing_data(card)
                self.vehicles.append(vehicle)
            
            # Intentar ir a siguiente p√°gina
            try:
                next_button = self.driver.find_element(By.CLASS_NAME, 'pagination-next')
                if next_button.is_enabled():
                    next_button.click()
                    time.sleep(2)  # Esperar carga
                    page += 1
                else:
                    break
            except:
                break
        
        print(f"‚úÖ Total veh√≠culos encontrados: {len(self.vehicles)}")
        return self.vehicles
    
    def extract_listing_data(self, card):
        """Extraer datos b√°sicos de una card de veh√≠culo"""
        # TODO: Ajustar selectores seg√∫n estructura real
        return {
            'listing_id': card.get('data-id') or card.find('a')['href'].split('/')[-1],
            'model': card.find('h3', class_='model-name').text.strip(),
            'price': self.parse_price(card.find('span', class_='price').text),
            'mileage': self.parse_mileage(card.find('span', class_='mileage').text),
            'year': card.find('span', class_='year').text.strip(),
            'url': card.find('a')['href'],
            'main_image': card.find('img')['src'],
            'dealership_location': card.find('span', class_='location').text.strip()
        }
    
    def scrape_vehicle_details(self, url):
        """Extraer datos detallados de una ficha individual"""
        self.driver.get(url)
        
        # Esperar carga completa
        WebDriverWait(self.driver, 20).until(
            EC.presence_of_element_located((By.CLASS_NAME, "vehicle-details"))
        )
        
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        
        # TODO: Ajustar selectores seg√∫n estructura real
        details = {
            'version': soup.find('h2', class_='version').text.strip(),
            'fuel_type': soup.find('span', class_='fuel').text.strip(),
            'transmission': soup.find('span', class_='transmission').text.strip(),
            'power_hp': self.parse_power(soup.find('span', class_='power').text),
            'exterior_color': soup.find('span', class_='color-exterior').text.strip(),
            'interior_color': soup.find('span', class_='color-interior').text.strip(),
            'extras': self.extract_extras(soup),
            'images': [img['src'] for img in soup.find_all('img', class_='gallery-image')],
            'dealership_name': soup.find('span', class_='dealer-name').text.strip(),
        }
        
        return details
    
    def extract_extras(self, soup):
        """Extraer lista de extras del veh√≠culo"""
        extras_section = soup.find('div', class_='extras-list')
        if not extras_section:
            return []
        
        extras = []
        for item in extras_section.find_all('li'):
            extras.append(item.text.strip())
        
        return extras
    
    def parse_price(self, price_str):
        """Convertir string de precio a n√∫mero"""
        # "32.990 ‚Ç¨" ‚Üí 32990
        return float(price_str.replace('‚Ç¨', '').replace('.', '').replace(',', '.').strip())
    
    def parse_mileage(self, km_str):
        """Convertir string de km a n√∫mero"""
        # "45.000 km" ‚Üí 45000
        return int(km_str.replace('km', '').replace('.', '').strip())
    
    def parse_power(self, power_str):
        """Convertir string de potencia a n√∫mero"""
        # "190 CV" ‚Üí 190
        return int(power_str.replace('CV', '').replace('cv', '').strip())
    
    def save_to_json(self, filename='bmw_premium_selection.json'):
        """Guardar resultados en JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.vehicles, f, ensure_ascii=False, indent=2)
        print(f"üíæ Datos guardados en {filename}")
    
    def close(self):
        """Cerrar navegador"""
        if self.driver:
            self.driver.quit()

# Uso
if __name__ == "__main__":
    scraper = BMWPremiumSelectionScraper()
    scraper.init_driver()
    
    try:
        # Extraer listados
        vehicles = scraper.scrape_listings()
        
        # Extraer detalles de cada veh√≠culo (OPCIONAL, puede ser lento)
        # for i, vehicle in enumerate(vehicles[:10]):  # Solo primeros 10 para prueba
        #     print(f"Extrayendo detalles {i+1}/{len(vehicles)}")
        #     details = scraper.scrape_vehicle_details(vehicle['url'])
        #     vehicle.update(details)
        #     time.sleep(1)  # Rate limiting
        
        # Guardar resultados
        scraper.save_to_json()
        
    finally:
        scraper.close()
```

### Scraper MINI Next

```python
# scrapers/mini_next_scraper.py

# Estructura similar a BMW Premium Selection
# Ajustar URL base y selectores seg√∫n estructura de MINI Next

class MiniNextScraper:
    def __init__(self):
        self.base_url = "https://www.mininext.es/buscador-avanzado"
        # ... resto similar a BMWPremiumSelectionScraper
```

---

## ‚ö†Ô∏è CONSIDERACIONES IMPORTANTES

### 1. Aspectos Legales
```
‚úÖ Verificar robots.txt:
   - https://www.bmw.es/robots.txt
   - https://www.mininext.es/robots.txt

‚úÖ Respetar t√©rminos de uso

‚úÖ Rate limiting:
   - M√°ximo 1 request cada 2-3 segundos
   - Pausas entre p√°ginas
   - No sobrecargar servidores

‚úÖ User-Agent:
   - Identificarse correctamente
   - No simular ser navegador humano de forma enga√±osa
```

### 2. Manejo de Errores
```python
try:
    # Scraping
except selenium.common.exceptions.TimeoutException:
    # Timeout esperando elemento
    log_error("Timeout cargando p√°gina")
except selenium.common.exceptions.NoSuchElementException:
    # Elemento no encontrado (estructura cambi√≥)
    log_error("Estructura de p√°gina cambi√≥")
except Exception as e:
    # Otros errores
    log_error(f"Error inesperado: {e}")
```

### 3. Cambios en la Web
```
‚ö†Ô∏è Las webs pueden cambiar su estructura
‚Üí Los selectores CSS/XPath dejar√°n de funcionar
‚Üí Necesario mantenimiento peri√≥dico del scraper

Soluci√≥n:
- Logs detallados de errores
- Alertas cuando scraper falla
- Revisi√≥n mensual de selectores
```

### 4. Captchas y Anti-Bot
```
Posibles medidas anti-scraping:
‚îú‚îÄ‚îÄ Captcha visual
‚îú‚îÄ‚îÄ reCAPTCHA
‚îú‚îÄ‚îÄ Rate limiting por IP
‚îî‚îÄ‚îÄ Detecci√≥n de headless browser

Soluciones:
‚îú‚îÄ‚îÄ Usar proxies rotativos (si necesario)
‚îú‚îÄ‚îÄ Selenium undetected-chromedriver
‚îú‚îÄ‚îÄ Pausas aleatorias entre requests
‚îî‚îÄ‚îÄ Respetar rate limits estrictos
```

---

## üìÖ FRECUENCIA DE SCRAPING

### Recomendado
```
BMW Premium Selection: 1 vez cada 24 horas
MINI Next: 1 vez cada 24 horas

Horario √≥ptimo: 02:00 - 04:00 AM
‚îú‚îÄ‚îÄ Menor tr√°fico en las webs
‚îú‚îÄ‚îÄ No interfiere con scrapers DUC/CMS (09:00-18:00)
‚îî‚îÄ‚îÄ Datos listos para an√°lisis por la ma√±ana
```

---

## üóÑÔ∏è TABLAS DE BASE DE DATOS

Ver archivo: `sql/create_bmw_mini_public_scrapers.sql`

---

## üì¶ PR√ìXIMOS PASOS

1. **Investigaci√≥n manual**: Inspeccionar webs reales para identificar selectores exactos
2. **Prototipo**: Desarrollar scraper b√°sico que extraiga 10 veh√≠culos de prueba
3. **Validaci√≥n**: Verificar calidad y completitud de datos extra√≠dos
4. **Integraci√≥n**: Conectar con Supabase y crear tablas
5. **Testing**: Ejecutar scraping completo en entorno de prueba
6. **Producci√≥n**: Integrar en CVO Scraper V1
7. **Automatizaci√≥n**: Programar ejecuci√≥n diaria

---

## üîó RECURSOS √öTILES

- Selenium Python Docs: https://selenium-python.readthedocs.io/
- BeautifulSoup Docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- Undetected ChromeDriver: https://github.com/ultrafunkamsterdam/undetected-chromedriver
- Article Web Scraping Coches: https://www.elladodelmal.com/2024/07/como-buscar-buenos-precios-en-coches-de.html

---

**Versi√≥n:** 1.0  
**Fecha:** 27/10/2025  
**Autor:** An√°lisis para integraci√≥n en CVO


